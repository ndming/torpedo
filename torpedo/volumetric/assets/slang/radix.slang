import splat;

[[vk::push_constant]]
uniform RasterInfo info;

[[vk::binding(6)]]
RWStructuredBuffer<uint64_t> keys;

[[vk::binding(7)]]
StructuredBuffer<uint32_t> vals;

[[vk::binding(8)]]
RWStructuredBuffer<uint64_t> sortedKeys;

[[vk::binding(9)]]
RWStructuredBuffer<uint32_t> indices;

static const uint RADIX_SORT_BINS = WORKGROUP_SIZE; // bins must be >= workgroup size
static const uint SORT_ITERATIONS = 8; // sorting 8 bits per iteration
static const uint SUBGROUP_SIZE = 32;  // 32 for NVIDIA, 64 if AMD

groupshared uint histogram[RADIX_SORT_BINS];
groupshared uint sums[RADIX_SORT_BINS / SUBGROUP_SIZE]; // sub-group reduction
groupshared uint localOffsets[RADIX_SORT_BINS]; // sub-group exclusive prefix sum
groupshared uint groupOffsets[RADIX_SORT_BINS]; // workgroup exclusive prefix sum

struct BinFlags {
    uint flags[WORKGROUP_SIZE / 32];
};
groupshared BinFlags binFlags[RADIX_SORT_BINS];

// Performs radix sort on the key/value pairs generated by keygen.slang, based on:
// https://github.com/embree/embree/blob/v4.0.0-ploc/kernels/rthwif/builder/gpu/sort.h
// https://github.com/MircoWerner/VkRadixSort/blob/main/singleradixsort/resources/shaders/single_radixsort.comp

[[ForceInline]]
uint64_t getElement(uint idx, uint iteration) {
    return iteration % 2 == 0 ? keys[idx] : sortedKeys[idx];
}

[shader("compute")]
[numthreads(WORKGROUP_SIZE, 1, 1)]
void main(uint3 localInvocationID  : SV_GroupThreadID) {
    let localID = localInvocationID.x;
    let laneID = WaveGetLaneIndex();
    let subgroupID = localID / WaveGetLaneCount();

    for (uint iteration = 0; iteration < SORT_ITERATIONS; iteration++) {
        let shift = 8 * iteration;

        // Initialize histogram
        if (localID < RADIX_SORT_BINS) {
            histogram[localID] = 0u;
        }
        GroupMemoryBarrierWithGroupSync(); // barrier()

        for (uint id = localID; id < info.tilesRendered; id += WORKGROUP_SIZE) {
            // Determine the bin
            let bin = uint(getElement(id, iteration) >> shift & uint64_t(RADIX_SORT_BINS - 1));
            // Increment the histogram
            InterlockedAdd(histogram[bin], 1u); // atomicAdd()
        }
        GroupMemoryBarrierWithGroupSync(); // barrier()

        // Subgroup reductions and subgroup prefix sums
        if (localID < RADIX_SORT_BINS) {
            let histogram_count = histogram[localID];
            let sum = WaveActiveSum(histogram_count); // subgroupAdd()
            uint prefix_sum = WavePrefixSum(histogram_count); // subgroupExclusiveAdd()
            localOffsets[localID] = prefix_sum;
            if (WaveIsFirstLane()) { // subgroupElect()
                // One thread inside the warp/subgroup enters this section
                sums[subgroupID] = sum;
            }
        }
        GroupMemoryBarrierWithGroupSync(); // barrier()

        // Workgroup prefix sums (offsets)
        if (subgroupID == 0) {
            uint offset = 0;
            for (uint i = laneID; i < RADIX_SORT_BINS; i += SUBGROUP_SIZE) {
                groupOffsets[i] = offset + localOffsets[i];
                offset += sums[i / SUBGROUP_SIZE];
            }
        }
        GroupMemoryBarrierWithGroupSync(); // barrier()

        // Scatter keys according to global offsets
        let flagsBin = localID / 32;
        let flagsBit = 1u << (localID % 32);

        for (uint blockID = 0; blockID < info.tilesRendered; blockID += WORKGROUP_SIZE) {
            GroupMemoryBarrierWithGroupSync(); // barrier()
            let id = blockID + localID;

            // Initialize bin flags
            if (localID < RADIX_SORT_BINS) {
                for (uint i = 0; i < WORKGROUP_SIZE / 32; i++) {
                    binFlags[localID].flags[i] = 0u; // init all bin flags to 0
                }
            }
            GroupMemoryBarrierWithGroupSync(); // barrier()

            uint64_t elementIn = 0;
            uint binID = 0;
            uint binOffset = 0;
            if (id < info.tilesRendered) {
                elementIn = getElement(id, iteration);
                binID = uint(elementIn >> shift & uint64_t(RADIX_SORT_BINS - 1));
                // Offset for group
                binOffset = groupOffsets[binID];
                // Add bit to flag
                InterlockedAdd(binFlags[binID].flags[flagsBin], flagsBit);
            }
            GroupMemoryBarrierWithGroupSync(); // barrier()

            if (id < info.tilesRendered) {
                // Calculate output index of element
                uint prefix = 0;
                uint count = 0;
                for (uint i = 0; i < WORKGROUP_SIZE / 32; i++) {
                    const uint bits = binFlags[binID].flags[i];
                    const uint fullCount = countbits(bits); // bitCount()
                    const uint partialCount = countbits(bits & (flagsBit - 1)); // bitCount()
                    prefix += (i <  flagsBin) ? fullCount : 0U;
                    prefix += (i == flagsBin) ? partialCount : 0U;
                    count += fullCount;
                }
                if (iteration % 2 == 0) {
                    sortedKeys[binOffset + prefix] = elementIn;
                } else {
                    keys[binOffset + prefix] = elementIn;
                }
                indices[binOffset + prefix] = vals[id];
                if (prefix == count - 1) {
                    InterlockedAdd(groupOffsets[binID], count);
                }
            }
        }
    }
}